{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Techne ML workbook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xmeGWAA8o1Li",
        "qkPK2J2FMQn-"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXn8ps5DUxMz1cpycQQUa+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mark-bell-tna/TechneTraining/blob/main/Code/Techne_ML_workbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7bE_8PPoV5k"
      },
      "source": [
        "## Set up variables and install useful library code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkwzM1cf5U0-"
      },
      "source": [
        "import sys\n",
        "data_source = \"Github\"\n",
        "!git clone https://github.com/nationalarchives/TechneTraining.git\n",
        "sys.path.insert(0, 'TechneTraining')\n",
        "sys.path.insert(0, 'TechneTraining/Code')\n",
        "github_data = \"TechneTraining/Data/\"\n",
        "import techne_library_code as tlc\n",
        "from IPython.display import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmeGWAA8o1Li"
      },
      "source": [
        "## Load word list from Topic Model built on 'regulation' related websites.\n",
        "Display words in table.\n",
        "\n",
        "A topic model is created from a corpus of text by an unsupervised machine learning algorithm. The process is non-deterministic which means the results will differ every time it is run. Below is on results from running the software MALLET over the 'regulation' corpus.\n",
        "\n",
        "The primary output is a list of topics (in this case 8, one row each) and a list of words most representative of that topic. A word can appear in more than one topic.\n",
        "\n",
        "From this we can get a high level overview of a corpus of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8ex1oI6pyO"
      },
      "source": [
        "topic_words = tlc.read_topic_list(github_data + \"TM/topic_list.txt\")\n",
        "TOPICS = len(topic_words)\n",
        "topic_table = tlc.pd.DataFrame([v[0:12] for v in topic_words.values()])\n",
        "topic_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtBZaBd4wSgX"
      },
      "source": [
        "Stop words are used in Natural Language Processing to filter out very common words, or those which may negatively affect the results.\n",
        "\n",
        "Below is a list of example stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Ict_D76URX"
      },
      "source": [
        "stop_words = [\"i\",\"or\",\"which\",\"of\",\"and\",\"is\",\"the\",\"a\",\"you're\",\"you\",\"at\",\"his\",\"etc\",'an','where','when']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8qvSbu5wpZ3"
      },
      "source": [
        "We can add more to this list by selecting from the following list. Which ones do you think might be worth filtering out?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Pfvyjn9pWV"
      },
      "source": [
        "additional_stops = ['medical','freight','pdf','plan','kb','regulation','risk']\n",
        "stop_word_select = tlc.widgets.SelectMultiple(options=additional_stops, rows=len(additional_stops))\n",
        "stop_word_select"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMwV5ZV8n4NT"
      },
      "source": [
        "for w in stop_word_select.value:\n",
        "    print(\"Adding\",w,\"to stop words\")\n",
        "    stop_words.append(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQt0SHFVw5tU"
      },
      "source": [
        "As well as a list of topics and related words, MALLET also produces a topic breakdown for each document in the corpus.\n",
        "\n",
        "Here we load the topic data and visualise some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTyOifKFBTzS"
      },
      "source": [
        "topics_per_doc = tlc.read_doc_topics(github_data + \"TM/topics_per_doc.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M_WaCclxQfe"
      },
      "source": [
        "The following plots show the proportion of each topic attributed to 4 different documents.\n",
        "\n",
        "\n",
        "\n",
        "1.   Top Left: One topic clearly dominates\n",
        "2.   Top Right: One dominant topic but a second topic is above the level of others\n",
        "3.   Bottom Left: Two topics clearly above others\n",
        "4.   Bottom Right: Topics close to being even"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EixqerZiB0hG"
      },
      "source": [
        "file_number_list = [212, 85, 9, 372]\n",
        "fig, ax = tlc.plot_doc_topics(file_number_list, topics_per_doc, TOPICS)\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27umRRru2RXo"
      },
      "source": [
        "## From Topics to Classes\n",
        "\n",
        "For this Machine Learning exercise we want to predict a Category of regulation (e.g. \"Medicine\" or \"Rail\"). The categories we may want to predict do not map one-to-one with the topics above. So first we need to create that mapping.\n",
        "\n",
        "Firstly we will define a list of possible categories. Sometimes the topics that come out may be worth ignoring (e.g. cookie information) but in this case all of them seem to be of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U-KX4MGEWEV"
      },
      "source": [
        "topics_of_interest = [0,1,2,3,4,5,6,7]\n",
        "class_names = {0:\"General\",1:\"Medicine\",2:\"Rail\",3:\"Safety\",4:\"Pensions\",5:\"Education\",6:\"Other\",-1:\"Unclassified\"}\n",
        "topic_to_class = {}\n",
        "topic_to_class = {0:1,1:0,2:2,3:3,4:4,5:4,6:2,7:2}  #For testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6QYWQ9s2_cE"
      },
      "source": [
        "Using the dropdown and list selector below we can set the mapping from topic to Class (a term more commonly used in machine learning for category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZWRIL6YhwWy"
      },
      "source": [
        "class_drop = tlc.widgets.Dropdown(options=[(v,k) for k,v in class_names.items()], value=topics_of_interest[0], rows = len(class_names))\n",
        "topic_select = tlc.widgets.SelectMultiple(options=[(w[0:5],t) for t,w in topic_words.items()],\n",
        "                                      value = [k for k,v in topic_to_class.items() if v == class_drop.value],\n",
        "                                      rows = TOPICS+1, height=\"100%\")\n",
        "\n",
        "button = tlc.widgets.Button(description=\"Update\")\n",
        "output = tlc.widgets.Output()\n",
        "\n",
        "#D = display(button, output)\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    for v in topic_select.value:\n",
        "        topic_to_class[v] = class_drop.value\n",
        "        print(\"Updated\")\n",
        "        #with output:\n",
        "        #    print(\"Mapped\",topic_to_class[v],\"to\",class_drop.value)\n",
        "\n",
        "button.on_click(on_button_clicked)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eojvFzQArUtm"
      },
      "source": [
        "V = tlc.widgets.VBox([class_drop, topic_select, button])\n",
        "V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHbuMTQA3Tna"
      },
      "source": [
        "Update the selected values here and then return to the dropdown until finished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqcvHHmx389I"
      },
      "source": [
        "Display the resulting mappings from **Topic** to **Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBOjQl1lkGFV"
      },
      "source": [
        "tlc.pd.DataFrame([(\",\".join(topic_words[k][0:10]),class_names[v]) for k,v in topic_to_class.items()], columns=['Topic Words','Class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJofXEU4CNmV"
      },
      "source": [
        "classes_per_doc = tlc.topic_to_class_scores(topics_per_doc, topic_to_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUn4IVn1yjRU"
      },
      "source": [
        "Generally every document contains a bit of each topic. Before visualising the class breakdown for our sample documents, we can filter out lowest scoring classes and focus on the primary class(es) by zeroing all values below a threshold. We then **normalise** the probabilities to add to 1.\n",
        "\n",
        "Run the next piece of code to create a slider to set the threshold, and then the following one will draw graphs. To try a different threshold, adjust the slider and rerun the graph code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQHy9DSofo9i"
      },
      "source": [
        "class_threshold = tlc.widgets.FloatSlider(0.10, min=0.10, max=0.65, step=0.05)\n",
        "class_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-R1Rzx1NQ8E"
      },
      "source": [
        "This is the graph code. It shows Classes above the threshold defined by the slider for the four documents previously visualised. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAf_o1deYGFE"
      },
      "source": [
        "filtered_classes_per_doc = tlc.filter_topics_by_threshold(classes_per_doc, class_threshold.value)\n",
        "class_count = len(filtered_classes_per_doc['file_1.txt'])\n",
        "fig, ax = tlc.plot_doc_topics(file_number_list, filtered_classes_per_doc, class_count, normalise=True)\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBFtHK7LtQ6H"
      },
      "source": [
        "Now that we've mapped topics and categories, it is time to prepare the text corpus (the document contents) for Machine Learning.\n",
        "\n",
        "First we load the content from a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVzM6PH2Qfre"
      },
      "source": [
        "file_contents = tlc.load_content(github_data + \"TM/tm_file_contents.txt\")\n",
        "\n",
        "file_list = []\n",
        "corpus = []\n",
        "\n",
        "for k,v in file_contents.items():\n",
        "    file_list.append(k)\n",
        "    corpus.append(tlc.clean_string(v))\n",
        "\n",
        "file_to_idx = dict([(x,i) for i,x in enumerate(file_list)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkPK2J2FMQn-"
      },
      "source": [
        "## Representing text as numbers\n",
        "\n",
        "The next stage is to use the results of the topic modelling to train a Supervised Learning algorithm.\n",
        "\n",
        "Supervised Learning is learning by example. We label our data in advance with categories, and then the algorithm derives a function which will map an input data item to an output Class. The input data is usually termed **Features**, the outputs are often called **Responses**.\n",
        "\n",
        "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "For this exercise the features are the words in our documents. There is no semantic meaning attached, the words are no more than tokens. Imagine a spreadsheet where each row represents a document and each column represents a word from a fixed vocabulary.\n",
        "\n",
        "One representation we could use would be to use a 1 to indicate a word appears in a document, and a 0 if it doesn't. This is simple but overly so. A better representation is TF-IDF which stands for Term Frequency-Inverse Document Frequency. It is a very simple idea but the general gist is that a word that appears in most documents will score lowly, while a word which appears in few documents will score highly (this is the Inverse Document Frequency part). The Term Frequency increases the score when it appears many times in the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG1x_8PX0ARl"
      },
      "source": [
        "We have some influence over the parameters used to define the TFIDF representation of our corpus. How they are set can influence the results.\n",
        "\n",
        "1. Features: how many distinct words from the corpus to use for the Vocabulary\n",
        "2. Min Doc Frequency: minimum number of documents a word must appear in to be considered\n",
        "3. Max Doc Frequency: maximum number of documents a word must appear in to be considered\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OQfouoH6IWh"
      },
      "source": [
        "FEATURES=1000\n",
        "MIN_DOC_FREQ=4\n",
        "MAX_DOC_FREQ=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNTD8JnYniM0"
      },
      "source": [
        "vectorizer = tlc.TfidfVectorizer(max_features=FEATURES, min_df=MIN_DOC_FREQ, max_df=MAX_DOC_FREQ, stop_words = stop_words)\n",
        "TFIDF = vectorizer.fit_transform(corpus)\n",
        "print(\"Documents:\",TFIDF.shape[0],\"\\tWords\",TFIDF.shape[1])\n",
        "training_files, training_features, training_class = tlc.prepare_for_ml(TFIDF, classes_per_doc, file_to_idx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzWDC92fO4AT"
      },
      "source": [
        "vocabulary = vectorizer.get_feature_names()\n",
        "example_row = TFIDF[file_number_list[2]]\n",
        "example_table = tlc.pd.DataFrame(zip([vocabulary[w] for w in example_row.nonzero()[1]],\n",
        "                                 [int(example_row[0,v]*1000)/1000 for i,v in enumerate(example_row.nonzero()[1])]),\n",
        "                                 columns = ['word','tfidf'])\n",
        "example_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh4ctvijlkBG"
      },
      "source": [
        "## Supervised Machine Learning\n",
        "\n",
        "For this exercise we will use the Naive Bayes algorithm. It is called Bayes because it uses Bayesian probability (named after the Reverend Thomas Bayes who discovered it). It is Naive because it assumes that all words in a document are independent of each other (think of the sentence \"my cat miaows when hungry\"). It seems like a bad assumption but actually works well in practice.\n",
        "\n",
        "Bayesian probability is surprisingly simple and gives us the ability to flip probabilities around. From a corpus of text I can easily calculate the probability of the word \"Passenger\" appearing in a document about \"Railways\", and also in a document about \"Medicines\". What Bayes' rule does is allow me to then calculate the probability that a document is about \"Railways\" or \"Medicines\" given that it contains the word \"Passenger\". Very handy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGlymMDBn8Yv"
      },
      "source": [
        "Before starting any Machine Learning we need to split our data into Training and Test datasets. The reason for this is that algorithms can appear to perform very well against the dataset they were trained with, but then perform very badly on new, unseen, data.\n",
        "\n",
        "The algorithm will learn from the Training data and then we check its performance against the Test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhlX9S2veMqJ"
      },
      "source": [
        "TEST_PROPORTION = 0.6\n",
        "X_train, X_test, y_train, y_test, f_file_train, f_file_test = tlc.train_test_split(training_features, training_class, training_files,\n",
        "                                                                             test_size = TEST_PROPORTION, random_state=42, stratify=training_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXa4FQ09p_Xd"
      },
      "source": [
        "Now we **fit** a Naive Bayes model to the training data. Two lines of code and it is done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6R9i3LafxH2"
      },
      "source": [
        "model = tlc.BernoulliNB()\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37DBRPqgx1iQ"
      },
      "source": [
        "Having created the model we now use it to generate predictions for the test dataset.\n",
        "\n",
        "We have two ways of assessing the performance of the model. The first is to give an accuracy score, quite simply the percentage of predictions it has got right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLiJ5d8Gnkia"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)\n",
        "print(\"Prediction Accuracy:\",int(tlc.accuracy_score(y_test, y_pred)*1000)/10,'%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjDX6w5ByYd4"
      },
      "source": [
        "A more granular method is to view what is quite rightly called a **Confusion Matrix**.\n",
        "\n",
        "A confusion matrix is a grid mapping 'correct' answers to predictions. The rows represent the class assigned in the test data and the columns represent predictions. The top left to bottom right diagonal shows us how many of each class have been predicted correctly. All of the other numbers count incorrect predictions. The number in row 2, column 3, will show how many documents of whatever the 2nd class represents (\"Medicine\") have been misclassified as the 3rd class (\"Rail\").\n",
        "\n",
        "In this example we see that \"Rail\" documents tend to be classified correctly, but a lot of the other types are being also misclassified as \"Rail\".\n",
        "\n",
        "We can see from the numbers that this dataset is highly imbalanced, so most of the records are \"Rail\". This may be responsible for the bias towards that class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6HOTYBzylPl"
      },
      "source": [
        "fig, ax = tlc.draw_confusion(y_test, y_pred, model, class_names)\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rewyDFuGe6tk"
      },
      "source": [
        "y_true_pred = [x for x in zip(range(len(y_test)),y_test, y_pred, y_prob)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_1RTC4BfkQA"
      },
      "source": [
        "true_0_pred_2 = [(y[0],y[1],y[2],y[3]) for i,y in enumerate(y_true_pred) if y[1] in [0,2] and y[2] in [0,2] and y[1] != y[2]]\n",
        "[x for x in true_0_pred_2][0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gci-CKqmP76"
      },
      "source": [
        "prediction_sample = tlc.random.sample(range(len(true_0_pred_2)), min(5, len(true_0_pred_2)))\n",
        "fig, ax = tlc.pyplot.subplots(min(5,len(prediction_sample)),1)\n",
        "fig.set_size_inches(5,7)\n",
        "\n",
        "for i, sample_idx in enumerate(prediction_sample):\n",
        "    data_idx = true_0_pred_2[sample_idx][0]\n",
        "    tp = true_0_pred_2[sample_idx][3]\n",
        "    class_probs = [tp[0],tp[1],tp[2],tp[3],tp[4]]\n",
        "    ax[i].set_ylim([0,1.0])\n",
        "    #ax[int(i/2), i % 2].set_xlim([0,TOPICS-1])\n",
        "    #ax[i].title.set_text(str(idx))\n",
        "    ax[i].text(.4,0.8, str(data_idx),\n",
        "        horizontalalignment='center',\n",
        "        transform=ax[i].transAxes)\n",
        "    if i < 4:\n",
        "        ax[i].set_xticks([])\n",
        "    else:\n",
        "        ax[i].set_xticks(ticks=[0,1,2,3,4])\n",
        "        ax[i].set_xticklabels(labels=['General','Medicine','Rail', 'Safety', 'Pension'])\n",
        "    ax[i].bar(x = [0,1,2,3,4], height = class_probs)\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAHI_ke3tIx4"
      },
      "source": [
        "uncertains = [x for x in true_0_pred_2 if max(x[3][1], x[3][0]) < 0.95]\n",
        "for u in uncertains:\n",
        "    print([u[0],u[1],u[2],[int(y*100)/100 for y in u[3]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ksyQlzLvHMu"
      },
      "source": [
        "kdt = tlc.KDTree(training_features, leaf_size=30, metric='euclidean')\n",
        "tfidf_words = vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHpduLyjvQnf"
      },
      "source": [
        "this_idx = uncertains[0][0]\n",
        "this_file = file_list[this_idx]\n",
        "print(file_contents[this_file])\n",
        "this_words = set([tfidf_words[w] for w in tlc.np.nonzero(training_features[this_idx])[1]])\n",
        "print(this_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3kyDT8q0YET"
      },
      "source": [
        "\n",
        "prob_match_sum = tlc.np.zeros((11,4))\n",
        "#print(prob_match_sum.shape)\n",
        "for i in range(11):\n",
        "    prob_match_sum[i,0] = i/10\n",
        "for row in y_true_pred:\n",
        "    max_prob = int(max(row[3]) * 10)\n",
        "    #if prob_match_sum[max_prob, 0] == 0:\n",
        "    #    prob_match_sum[max_prob, 0] = max_prob/10\n",
        "    prob_match_sum[max_prob,int(row[1] == row[2])+1] += 1\n",
        "    prob_match_sum[max_prob,3] += 1\n",
        "\n",
        "#for x in max_sorted_asc[0:5]:\n",
        "#    print(x)\n",
        "\n",
        "#print(\"\")\n",
        "#for x in max_sorted_desc[0:5]:\n",
        "#    print(x)\n",
        "\n",
        "prob_match_sum = tlc.pd.DataFrame(prob_match_sum, columns=[\"Probability\", \"NoMatch\", \"Match\", \"Total\"])\n",
        "#prob_match_sum\n",
        "\n",
        "ax = prob_match_sum.plot(x=\"Probability\", y=\"Total\", kind=\"bar\", color=\"blue\")\n",
        "ax.legend(['Disagree', 'Match'])\n",
        "prob_match_sum.plot(x=\"Probability\", y=\"Match\", kind=\"bar\", ax=ax, color=\"orange\")\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVFqo--f4l_6"
      },
      "source": [
        "max_sorted_asc = sorted(y_true_pred, key=lambda max_x : max(max_x[3]), reverse=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbihep_2xzA2"
      },
      "source": [
        "check_row = tlc.random.randint(0,100)\n",
        "print(\"Random number:\", check_row)\n",
        "print(max_sorted_asc[check_row])\n",
        "this_idx = max_sorted_asc[check_row][0]\n",
        "this_words = set([tfidf_words[w] for w in tlc.np.nonzero(training_features[this_idx])[1]])\n",
        "\n",
        "print(\"Doc Index:\", this_idx, \"Doc words:\", \",\".join(list(this_words)))\n",
        "nn_dist, nn_ind = kdt.query(training_features[this_idx], k=4)\n",
        "print(\"Contents:\", file_contents[file_list[this_idx]])\n",
        "print(\"\")\n",
        "for i in range(len(nn_dist[0])):\n",
        "    dist = nn_dist[0][i]\n",
        "    idx = nn_ind[0][i]\n",
        "    if idx == this_idx:\n",
        "        continue\n",
        "    pred = model.predict(training_features[idx])\n",
        "    true_class = -1\n",
        "    if file_list[idx] in class_file_scores:\n",
        "        true_class = tlc.np.argmax(class_file_scores[file_list[idx]])\n",
        "    print(\"Match index:\", idx, \"; Predicted as:\", class_names[pred[0]],\n",
        "          \"; Labelled as:\", class_names[true_class],\n",
        "          \"; Distance score:\",dist)\n",
        "    words = set([tfidf_words[w] for w in tlc.np.nonzero(feature_matrix[idx])[1]])\n",
        "    print(\"\\tWords:  \", \",\".join(list(words)))\n",
        "    print(\"\\tOverlap:\", \",\".join(list(words.intersection(this_words))))\n",
        "    print(\"\\tContent:\",file_contents[file_list[idx]].strip())\n",
        "    print(\"\")\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}